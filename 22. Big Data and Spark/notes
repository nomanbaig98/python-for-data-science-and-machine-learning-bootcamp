Hadoop:
Apache Hadoop is an open source framework that is used to efficiently store and process large datasets ranging in size from gigabytes to petabytes of data.
Instead of using one large computer to store and process the data, Hadoop allows clustering multiple computers to analyze massive datasets in parallel more quickly.
Retailers use it to help analyze structured and unstructured data to better understand and serve their customers.


MapReduce:
MapReduce facilitates concurrent processing by splitting petabytes of data into smaller chunks, and processing them in parallel on Hadoop commodity servers. In the end, it aggregates all the data from multiple servers to return a consolidated output back to the application.


Apache Spark:
Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters. It does not have its own storage system, but runs analytics on other storage systems like HDFS, or other popular stores like Amazon Redshift, Amazon S3, Couchbase, Cassandra, and others.


PySpark:
PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment.


RDD Transformation:
 Filter : Applies the function to each element and return that element which evaluates to true.
 Map : transform each element and preserve it. works like pandas.apply()
 FlatMap : Transform a list of text into a list of words.
 
 
RDD Actions:
 First : Return the first element in the RDD.
 Collect : Return all the elements of RDD as an array.
 Count : Return the number of elements in the RDD
 Take : Return array with first n elements of the RDD, Here n is the argument provided.
 

